[Week 4–2 | Slide 1]
Meme from The Office humorously referencing linear regression — one character yells to stop using it, while another defiantly vows to use it “even harder.”

[Week 4–2 | Slide 3]
Mathematical formula showing the gradient-descent parameter updates for weights (wⱼ) and bias (b): each is adjusted by subtracting α times its partial derivative of the cost function J.

[Week 4–2 | Slide 4 (1)]
Illustration of a deep neural network with input, multiple hidden, and output layers — demonstrating how neurons in each layer are fully connected.

[Week 4–2 | Slide 4 (2)]
Diagram of a single neuron (node) with labeled inputs, weights, bias, activation function f(z), and output a — visualizing Z = Σ wᵢ xᵢ + b.

[Week 4–2 | Slide 4 (3)]
Chart set comparing several activation functions: Sigmoid, tanh, ReLU, Leaky ReLU, Maxout, and ELU, each with formula and graph showing its shape.

[Week 4–2 | Slide 5 (1)]
Simplified repeat chart of activation functions (Sigmoid, tanh, ReLU, Leaky ReLU, Maxout, ELU) with their characteristic curves.

[Week 4–2 | Slide 5 (2)]
Graph of the Softmax activation function, showing output probability distribution ranging from 0 to 1 across the input domain.

[Week 4–2 | Slide 6]
Basic neural-network diagram with input, hidden, and output layers — each node connected to the next layer, representing a feed-forward structure.

[Week 4–2 | Slide 8]
Schematic of a multi-layer network demonstrating backpropagation — weight adjustments flow from the output layer back through the hidden layers.

[Week 4–2 | Slide 9]
Illustration of the 2024 Nobel Prize in Physics announcement awarded to John J. Hopfield and Geoffrey E. Hinton for their foundational discoveries in machine learning and neural networks.

[Week 4–2 | Slide 9 (1)]
Screenshot of the 1986 Nature paper “Learning Representations by Back-Propagating Errors” by Rumelhart, Hinton & Williams — the foundational work that introduced backpropagation to train neural networks.

[Week 4–2 | Slide 9 (2)]
Equation z = w^\top x + b = \sum_{i=1}^{d} w_i x_i + b, showing the linear combination of inputs, weights, and bias that forms a neuron’s pre-activation value.

[Week 4–2 | Slide 9 (3)]
Diagram of a neuron (node) illustrating weighted inputs (x_1,x_2,x_3), weights (w_1,w_2,w_3), bias b, and the activation function f(z) that produces output a.

[Week 4–2 | Slide 9 (4)]
Formula a = \phi(z), defining how a neuron’s output is generated by applying an activation function \phi to its input sum z.

[Week 4–2 | Slide 11 (1)]
Partial-derivative chain-rule expression \frac{\partial L}{\partial w_i} = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial w_i}, used to derive gradients for each weight in backpropagation.

[Week 4–2 | Slide 11 (2)]
Similar gradient expression for bias: \frac{\partial L}{\partial b} = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial b}.

[Week 4–2 | Slide 11 (3)]
Definition L = \ell(a, y), where the loss function \ell compares predicted output a and true label y.

[Week 4–2 | Slide 11 (4)]
Bullet-point summary of partial derivatives in backpropagation: each term (\frac{\partial L}{\partial a}, \frac{\partial a}{\partial z}, \frac{\partial z}{\partial w_i}, \frac{\partial z}{\partial b}) depends on loss, activation, input, and bias.

[Week 4–2 | Slide 11 (5)]
Gradient-descent parameter-update rules:
w := w - \alpha \frac{\partial L}{\partial w},
b := b - \alpha \frac{\partial L}{\partial b}, showing learning rate \alpha adjustments to minimize loss.

[Week 4–2 | Slide 12 (1)]
Repeat of the weight and bias update equations emphasizing their role in the gradient-descent optimization loop.

[Week 4–2 | Slide 12 (1)]
Equation showing the chain rule for backpropagation: partial derivative of loss \frac{\partial L}{\partial w_{ij}^{(l)}} expressed as the product of derivatives with respect to the layer output, activation, and weights.

[Week 4–2 | Slide 12 (2)]
Three plots comparing model fit quality: underfitting with a nearly flat line, balanced fit matching the general trend, and overfitting with a highly oscillating curve.

[Week 4–2 | Slide 15 (1)]
Mathematical expression for L2 regularization, adding \lambda \sum w_j^2 to penalize large weight magnitudes.

[Week 4–2 | Slide 15 (2)]
Mathematical expression for L1 regularization, adding \lambda \sum |w_j| to promote sparsity in model weights.

[Week 4–2 | Slide 16]
Plot comparing L1 (|w|) and L2 (w²) penalty functions. L1 grows linearly while L2 grows quadratically, highlighting the difference in how they shrink weights.

[Week 4–2 | Slide 17]
Combined visual of both L1 and L2 regularization terms, showing how each modifies the loss function to control overfitting.

[Week 4–2 | Slide 18 (1)]
Gradient descent weight-bias update formulas:
w := w - \alpha \frac{\partial L}{\partial w}, b := b - \alpha \frac{\partial L}{\partial b}.

[Week 4–2 | Slide 18 (2)]
Diagram explaining Dropout regularization — random neurons (black nodes) are deactivated during training to prevent co-adaptation and improve generalization.

[Week 4–2 | Slide 18 (3)]
PyTorch documentation snippet defining the torch.nn.Dropout class, which randomly zeroes input elements with probability p during training.

[Week 4–2 | Slide 20]
Code example demonstrating Dropout in PyTorch: creating a layer with p=0.2, applying it to random input, and observing randomly dropped activations.

[Week 4-2 | Slide 21 (1)]
Graph showing training loss and validation loss over epochs. The validation loss begins to rise after a certain point, marking the optimal early stopping epoch where overfitting starts.

[Week 4-2 | Slide 21 (2)]
Dual-panel diagram labeled “Early Stopping.”
Left: validation and training loss curves diverge after the optimal epoch (“Stop training here!!”).
Right: validation accuracy peaks before training accuracy, indicating when to halt training.

[Week 4-2 | Slide 22 (1)]
Plot comparing underfitting, optimum, and overfitting regions as model complexity increases.
Generalization loss forms a U-shape, while training loss decreases steadily — illustrating the bias-variance tradeoff.