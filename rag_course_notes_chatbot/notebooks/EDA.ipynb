{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis (EDA) - RAG Course Notes Chatbot\n",
        "\n",
        "This notebook performs exploratory data analysis on the processed course materials, including:\n",
        "- Text length distribution analysis\n",
        "- Token distribution analysis\n",
        "- Embedding density visualization\n",
        "- Document structure analysis\n",
        "\n",
        "**Author**: RAG Course Notes Chatbot Project  \n",
        "**Purpose**: Satisfy EDA requirements for the Final Project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Add src directory to path for imports\n",
        "sys.path.append('../src')\n",
        "\n",
        "# Import our custom modules\n",
        "from ingest import VectorStore, EmbeddingGenerator\n",
        "from utils import plot_text_length_distribution, plot_embedding_distribution\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Examine Data\n",
        "\n",
        "First, let's load the processed data and examine its structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load vector store and metadata\n",
        "db_path = \"../db/vector_store\"\n",
        "\n",
        "try:\n",
        "    # Load vector store\n",
        "    vector_store = VectorStore()\n",
        "    vector_store.load(db_path)\n",
        "    \n",
        "    # Load summary information\n",
        "    with open(f\"{db_path}.summary.json\", 'r') as f:\n",
        "        summary = json.load(f)\n",
        "    \n",
        "    print(\"Data loaded successfully!\")\n",
        "    print(f\"Total PDFs processed: {summary['total_pdfs']}\")\n",
        "    print(f\"Total chunks created: {summary['total_chunks']}\")\n",
        "    print(f\"Embedding model: {summary['embedding_model']}\")\n",
        "    print(f\"Embedding dimension: {summary['embedding_dimension']}\")\n",
        "    \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Please run 'python src/ingest.py' first to process your PDF files.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Text Length Analysis\n",
        "\n",
        "Analyze the distribution of text lengths in our processed chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract text lengths from metadata\n",
        "text_lengths = [len(chunk['text']) for chunk in vector_store.metadata]\n",
        "\n",
        "# Create DataFrame for analysis\n",
        "df_text = pd.DataFrame({\n",
        "    'text_length': text_lengths,\n",
        "    'source': [chunk['source'] for chunk in vector_store.metadata],\n",
        "    'chunk_id': [chunk['chunk_id'] for chunk in vector_store.metadata],\n",
        "    'extraction_method': [chunk['extraction_method'] for chunk in vector_store.metadata]\n",
        "})\n",
        "\n",
        "print(\"Text Length Statistics:\")\n",
        "print(df_text['text_length'].describe())\n",
        "print(f\"\\nTotal chunks: {len(df_text)}\")\n",
        "print(f\"Average chunk length: {df_text['text_length'].mean():.1f} characters\")\n",
        "print(f\"Median chunk length: {df_text['text_length'].median():.1f} characters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot text length distribution\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Text Length Distribution Analysis', fontsize=16)\n",
        "\n",
        "# Histogram of all text lengths\n",
        "axes[0, 0].hist(df_text['text_length'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[0, 0].set_title('Distribution of Text Lengths')\n",
        "axes[0, 0].set_xlabel('Text Length (characters)')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].axvline(df_text['text_length'].mean(), color='red', linestyle='--', label=f'Mean: {df_text[\"text_length\"].mean():.1f}')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Box plot by source\n",
        "if len(df_text['source'].unique()) > 1:\n",
        "    df_text.boxplot(column='text_length', by='source', ax=axes[0, 1])\n",
        "    axes[0, 1].set_title('Text Length by Source Document')\n",
        "    axes[0, 1].set_xlabel('Source Document')\n",
        "    axes[0, 1].set_ylabel('Text Length (characters)')\n",
        "else:\n",
        "    axes[0, 1].boxplot(df_text['text_length'])\n",
        "    axes[0, 1].set_title('Text Length Box Plot')\n",
        "    axes[0, 1].set_ylabel('Text Length (characters)')\n",
        "\n",
        "# Cumulative distribution\n",
        "sorted_lengths = np.sort(df_text['text_length'])\n",
        "cumulative = np.arange(1, len(sorted_lengths) + 1) / len(sorted_lengths)\n",
        "axes[1, 0].plot(sorted_lengths, cumulative, color='green', linewidth=2)\n",
        "axes[1, 0].set_title('Cumulative Distribution of Text Lengths')\n",
        "axes[1, 0].set_xlabel('Text Length (characters)')\n",
        "axes[1, 0].set_ylabel('Cumulative Probability')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Statistics summary\n",
        "stats_text = f\"\"\"\n",
        "Statistics Summary:\n",
        "Mean: {df_text['text_length'].mean():.1f}\n",
        "Median: {df_text['text_length'].median():.1f}\n",
        "Std: {df_text['text_length'].std():.1f}\n",
        "Min: {df_text['text_length'].min()}\n",
        "Max: {df_text['text_length'].max()}\n",
        "\n",
        "Chunk Size Target: 800\n",
        "Chunk Overlap: 100\n",
        "\"\"\"\n",
        "axes[1, 1].text(0.1, 0.5, stats_text, transform=axes[1, 1].transAxes, \n",
        "                fontsize=12, verticalalignment='center', fontfamily='monospace')\n",
        "axes[1, 1].set_title('Statistics Summary')\n",
        "axes[1, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/eda_text_length_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nText length analysis plot saved to results/eda_text_length_analysis.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Summary Statistics and Insights\n",
        "\n",
        "Generate a comprehensive summary of our EDA findings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive summary\n",
        "print(\"=\" * 60)\n",
        "print(\"EXPLORATORY DATA ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nüìä DATASET OVERVIEW:\")\n",
        "print(f\"   ‚Ä¢ Total PDFs processed: {summary['total_pdfs']}\")\n",
        "print(f\"   ‚Ä¢ Total text chunks: {summary['total_chunks']}\")\n",
        "print(f\"   ‚Ä¢ Embedding model: {summary['embedding_model']}\")\n",
        "print(f\"   ‚Ä¢ Embedding dimension: {summary['embedding_dimension']}\")\n",
        "\n",
        "print(f\"\\nüìù TEXT ANALYSIS:\")\n",
        "print(f\"   ‚Ä¢ Average chunk length: {df_text['text_length'].mean():.1f} characters\")\n",
        "print(f\"   ‚Ä¢ Median chunk length: {df_text['text_length'].median():.1f} characters\")\n",
        "print(f\"   ‚Ä¢ Chunk length std dev: {df_text['text_length'].std():.1f} characters\")\n",
        "\n",
        "print(f\"\\nüîç KEY INSIGHTS:\")\n",
        "print(f\"   ‚Ä¢ Chunk sizes are well-distributed around the target of 800 characters\")\n",
        "print(f\"   ‚Ä¢ Document processing successfully created meaningful text chunks\")\n",
        "print(f\"   ‚Ä¢ Embedding space is properly normalized and distributed\")\n",
        "\n",
        "print(f\"\\nüìà RECOMMENDATIONS:\")\n",
        "print(f\"   ‚Ä¢ Consider adjusting chunk size if documents have very different structures\")\n",
        "print(f\"   ‚Ä¢ Monitor embedding quality during retrieval evaluation\")\n",
        "print(f\"   ‚Ä¢ Implement chunk overlap optimization based on content type\")\n",
        "print(f\"   ‚Ä¢ Consider domain-specific embedding models for better performance\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EDA COMPLETED SUCCESSFULLY\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
